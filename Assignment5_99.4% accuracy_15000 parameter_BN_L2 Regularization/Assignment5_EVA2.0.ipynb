{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment5_EVA2.0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aNyZv-Ec52ot"
      },
      "source": [
        "Assignment 5\n",
        "============\n",
        "\n",
        "Name : Nihar Kanungo\n",
        "Batch : 6:30 AM , Monday\n",
        "\n",
        "\n",
        "**Background :**\n",
        "- - - - - - - -\n",
        "This is a simple Image Recognition program which makes use of the MNIST preprocessed dataset to process the handwritten digit images and predict the numerical digit each image represents . The Code uses one of the most popular Tensorflow API Keras to perform the operations .It's a supervised Computer Vision problem.\n",
        "\n",
        "The reason behind choosing MNIST as our first dataset to perform our experiments is not because everyone else are using it but for the following reasons .\n",
        "\n",
        "1. The Amount of time it takes to train is less , hence we don't have to wait for a longer time to see the outcome of each improvement that we are trying to make  as first few assignments are those in which we perform experiments to sharpen our understanding of the basics \n",
        "\n",
        "2. As it's not much complicated dataset , hence we can train the model till we drop the image size till 5 x 5 and still it doesn't harash us by giving a less prediction. This would benefit us by understanding the complete image processing using Convolutional Neural Network \n",
        "\n",
        "3. As we plan to add improvements step by step to verify our theoritical understanding with practical experiments , hence a simple dataset will not change drastically due to change in initialization of the random parameters , thereby giving us some significance and visible change over the last change \n",
        "\n",
        "4. As to build our fundamentals we need to first work with images where the size of the image is same as size of the object with very less pixels around the border \n",
        "\n",
        "-----There are many other benefits of using MNIST but we will pause this topic here and will elaborate when necessary-----\n",
        "\n",
        "\n",
        "**There are 4 different networks defined in this file . Each network defined in this file is an improvement over it's Predecessor. **\n",
        "\n",
        "Network -1 : This is the Basic Network which defines the template over which the improvements will be added. The Basic network here refers to the final code of Assignment -4 \n",
        "\n",
        "Network -2 : This Network works on the Normalized image after the images are standardized  Network-1\n",
        "\n",
        "Network -3 : This Network adds custom loss functin which is L2 regularization added to the cross entropy loss to penalize the bigger weights \n",
        "\n",
        "Network -4 : This Network reverses the order in which batch normalization and activation function was used. In this network we are using batch normalization before the activation function but after the convolution. That means the activation is currently happening external to the convolution function \n",
        "\n",
        "\n",
        "\n",
        "**Input**\n",
        "- - - -\n",
        "1) 60000 Handwritten digit images (between 0-9)\n",
        "\n",
        "2) The Images are already segreegated as Train and Test Data with the respective target values\n",
        "\n",
        "\n",
        "**Environment**\n",
        "- - - - - - - - \n",
        "\n",
        "    Development - Colab GPU , Jupyter Notebook\n",
        "    Repository : Github\n",
        "\n",
        "**Algorithm**\n",
        "- - - - - - \n",
        "    Linear Model \n",
        "    Convolutional Neural Network (2D) - Gray Scale images\n",
        "    Maxpooling \n",
        "    Softmax Activation function\n",
        "    loss Function : Categorical Crossentropy\n",
        "    Optimizer=Adam\n",
        "    Metrics=accuracy\n",
        "    Batch Normalization\n",
        "    Drop Out \n",
        "    Image Normalization\n",
        "    Customized Loss function (Loss function  + L2 Regularization)\n",
        "\n",
        "\n",
        "**Parameters**\n",
        "- - - - - - ---------------------\n",
        "\n",
        "    Batch Size - Variable \n",
        "    Epochs - Variable\n",
        "    Kernel Size - Variable (Advisable to use 3 * 3)\n",
        "    Number of Kernels - Variable \n",
        "    Learning Rate\n",
        "    regularization coefficient\n",
        "    \n",
        "\n",
        "**Conditions**\n",
        "- - - - - -\n",
        "\n",
        "1. The Number of parameters < 15,000\n",
        "2. Should use only Conv2D\n",
        "3. Should not have applied Maxpooling before 2-4 layers of the conversion into number of classes (10 in this case)\n",
        "4. Maxpooling should be applied on receptive field of at least 5 x 5 or 7 x 7\n",
        "5. Activation function should be relu on conv 2D\n",
        "6. With < 15 EPochs\n",
        "7. Image Normalization should be used first\n",
        "8. L2 regularization should not be added to each layer \n",
        "9. Batch Normalization should be used before the Activation function\n",
        "\n",
        "**Expected Result**\n",
        "- - - - - - - - -\n",
        "\n",
        "1.To get >= 99.4 % accuracy \n",
        "\n",
        "2.To find the first 25 misclassified images\n",
        "\n",
        "3.Save the Best model w.r.t the Validation accuracy\n",
        "\n",
        "4.Save the model in the Google drive for future use \n",
        "\n",
        "5.To see that the Custom loss function tries to generalize better ( here we have taken a small lambda value , more such experiments may show significant improvements)\n",
        "\n",
        "6.To see that the Image Normalization improves performance ( Here we may not see a significant improvement as MNIST is a easy dataset, but it definitely helps to converge faster - I will try to run multiple times and post the best network )\n",
        "\n",
        "7.To see that the sequence of using Batch Normalization and Activation function don't have any impact on the output data which will be fed to the next layer as input \n",
        "\n",
        "8.To use Activation function outside of the Convolution layer \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A_JGIXQ8EAO_"
      },
      "source": [
        "# **Import Libraries and modules**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3m3w1Cw49Zkt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41608080-2653-4b75-de5c-a630f5d27bc5"
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Eso6UHE080D4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "92c864db-8365-47ac-aeab-b03c7cd89c55"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add,BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "from IPython.display import Image\n",
        "from keras.optimizers import Adam, SGD, Nadam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "import sys\n",
        "import dill\n",
        "import tensorflow as tf\n",
        "slim = tf.contrib.slim\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0820 14:44:56.024041 140643382257536 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zByEi95J86RD"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7eRM0QWN83PV",
        "outputId": "45a7239e-d9ed-4347-e6c2-42dfdd01074a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1hzKCdZmEAPM"
      },
      "source": [
        "# Print Metadata Information \n",
        "1. Print the shape of the Training Dataset (Number of Images, Size of the images)\n",
        "2. Import one of the popular library to plot charts/graphs and command to display it inline\n",
        "3. Code to show the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4a4Be72j8-ZC",
        "outputId": "09d52191-5862-4aee-cdca-001cfb4418de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "\n",
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe9bc888dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WDO4QtQTEAPR"
      },
      "source": [
        "#Reshape the Training and test data to gray scale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dkmprriw9AnZ",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PGJ9Z5rLEAPV"
      },
      "source": [
        "# Perform Data Standardization \n",
        "1. Convert the Images into float32 format\n",
        "2. Divide the values by 255 to make it with in 0-1 (Standardize)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X2m4YS4E9CRh",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzeZYL7TwwA9",
        "colab_type": "text"
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZG8JiXR39FHC",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AV85Am_ZEAPm"
      },
      "source": [
        "***The Core Setup for this assignment ( this is the final code of assignment No 4 - Recap)***\n",
        "=====================\n",
        "This core setup is going to be the basic framework . All the future codes will be improvements over this code but the basic setup will be unaltered . This would help us to understand the impact of each feature on our network. The Core setup is described below.\n",
        "\n",
        "1. Import the Activation layer \n",
        "2. Declare the Sequential model\n",
        "3. Add multiple convolution layers to increase the receptive field ( 3 x 3 kernels)\n",
        "4. Add Maxpooling to reduce the image size and increase the receptive field \n",
        "5. Use small dropout after each convolution to ensure that the model is not over fitting\n",
        "6. Add Batch normalization after activation of each layer to essure that the model is generalizing well\n",
        "7.Modify the learning rate after each epoch to converge \n",
        "5. Add a large kernel of size 5 x 5 at this stage as going below this may not show significant details of the images\n",
        "6. Flatten the data to make it 1D \n",
        "7. Apply Softmax on each output to find out the predicted class\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lnDRKPvFEAPq"
      },
      "source": [
        "Display the Summary of the Model. This would give the detail parameters each layer uses . This would be a great information to find out how the memory will be used and where are oppertunities for fine tuning.\n",
        "\n",
        " The Total parameters used for this network is < 15000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9iSYhmXnJwRX",
        "outputId": "1c81e050-baa0-4a5d-b2a3-be4e513f0030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(14, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Convolution2D(8, 1, 1, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 5))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=32, epochs=15, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred[:9])\n",
        "print(y_test[:9])\n",
        "\n"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_154 (Conv2D)          (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_103 (Bat (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_103 (Dropout)        (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_155 (Conv2D)          (None, 24, 24, 14)        1022      \n",
            "_________________________________________________________________\n",
            "batch_normalization_104 (Bat (None, 24, 24, 14)        56        \n",
            "_________________________________________________________________\n",
            "dropout_104 (Dropout)        (None, 24, 24, 14)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_156 (Conv2D)          (None, 22, 22, 24)        3048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 11, 11, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_157 (Conv2D)          (None, 11, 11, 8)         200       \n",
            "_________________________________________________________________\n",
            "conv2d_158 (Conv2D)          (None, 9, 9, 16)          1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_105 (Bat (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_105 (Dropout)        (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_159 (Conv2D)          (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_106 (Bat (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_106 (Dropout)        (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_160 (Conv2D)          (None, 5, 5, 24)          3480      \n",
            "_________________________________________________________________\n",
            "batch_normalization_107 (Bat (None, 5, 5, 24)          96        \n",
            "_________________________________________________________________\n",
            "dropout_107 (Dropout)        (None, 5, 5, 24)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_161 (Conv2D)          (None, 5, 5, 10)          250       \n",
            "_________________________________________________________________\n",
            "batch_normalization_108 (Bat (None, 5, 5, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_108 (Dropout)        (None, 5, 5, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_162 (Conv2D)          (None, 1, 1, 10)          2510      \n",
            "_________________________________________________________________\n",
            "flatten_18 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,430\n",
            "Trainable params: 14,254\n",
            "Non-trainable params: 176\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/15\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 35s 586us/step - loss: 0.1670 - acc: 0.9474 - val_loss: 0.0528 - val_acc: 0.9829\n",
            "Epoch 2/15\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 25s 420us/step - loss: 0.0664 - acc: 0.9794 - val_loss: 0.0453 - val_acc: 0.9862\n",
            "Epoch 3/15\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 25s 417us/step - loss: 0.0532 - acc: 0.9838 - val_loss: 0.0305 - val_acc: 0.9907\n",
            "Epoch 4/15\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0450 - acc: 0.9855 - val_loss: 0.0245 - val_acc: 0.9917\n",
            "Epoch 5/15\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 25s 415us/step - loss: 0.0382 - acc: 0.9875 - val_loss: 0.0275 - val_acc: 0.9913\n",
            "Epoch 6/15\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0352 - acc: 0.9890 - val_loss: 0.0226 - val_acc: 0.9924\n",
            "Epoch 7/15\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 25s 413us/step - loss: 0.0315 - acc: 0.9899 - val_loss: 0.0263 - val_acc: 0.9907\n",
            "Epoch 8/15\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 25s 416us/step - loss: 0.0293 - acc: 0.9905 - val_loss: 0.0273 - val_acc: 0.9910\n",
            "Epoch 9/15\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 25s 416us/step - loss: 0.0264 - acc: 0.9915 - val_loss: 0.0194 - val_acc: 0.9936\n",
            "Epoch 10/15\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 25s 412us/step - loss: 0.0252 - acc: 0.9917 - val_loss: 0.0220 - val_acc: 0.9928\n",
            "Epoch 11/15\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 25s 418us/step - loss: 0.0235 - acc: 0.9923 - val_loss: 0.0202 - val_acc: 0.9938\n",
            "Epoch 12/15\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 25s 411us/step - loss: 0.0224 - acc: 0.9928 - val_loss: 0.0212 - val_acc: 0.9939\n",
            "Epoch 13/15\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 24s 408us/step - loss: 0.0216 - acc: 0.9930 - val_loss: 0.0208 - val_acc: 0.9931\n",
            "Epoch 14/15\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 25s 410us/step - loss: 0.0203 - acc: 0.9936 - val_loss: 0.0224 - val_acc: 0.9935\n",
            "Epoch 15/15\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 25s 410us/step - loss: 0.0198 - acc: 0.9932 - val_loss: 0.0217 - val_acc: 0.9935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa1f919df60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        },
        {
          "output_type": "stream",
          "text": [
            "[0.021683135201784172, 0.9935]\n",
            "[[9.18202320e-12 4.11605328e-08 2.96076035e-08 2.18741363e-07\n",
            "  6.37922781e-08 2.65363093e-10 1.05903741e-12 9.99999046e-01\n",
            "  7.02135711e-11 6.27163956e-07]\n",
            " [1.02429249e-06 8.89295450e-07 9.99997377e-01 6.62932376e-09\n",
            "  2.20760811e-07 1.34024606e-12 3.47157510e-07 1.91765004e-09\n",
            "  6.49364367e-08 3.06864117e-10]\n",
            " [8.74688055e-08 9.99996185e-01 1.13632382e-07 2.02231703e-07\n",
            "  2.13324640e-07 2.87845296e-07 1.20585742e-06 1.48125787e-06\n",
            "  4.75331099e-08 1.40844083e-08]\n",
            " [9.99311328e-01 9.80048889e-11 1.34738873e-10 5.04616651e-08\n",
            "  6.14594731e-09 3.53703683e-07 6.86454761e-04 3.09342507e-09\n",
            "  5.02739226e-07 1.45698175e-06]\n",
            " [1.31976108e-09 2.90482705e-09 8.89440521e-10 3.39934331e-11\n",
            "  9.99985576e-01 1.42430095e-10 1.87326998e-09 7.39263717e-09\n",
            "  1.34966868e-07 1.42858698e-05]\n",
            " [5.87095244e-08 9.99988079e-01 1.61987060e-07 5.44403314e-08\n",
            "  8.42517579e-07 1.67542815e-08 2.98381423e-07 1.04503561e-05\n",
            "  5.54504709e-08 7.26151868e-08]\n",
            " [3.77705171e-14 3.41752127e-07 2.61288280e-09 8.65042649e-10\n",
            "  9.99910355e-01 3.33925199e-09 9.76861717e-11 6.02590408e-05\n",
            "  5.78013896e-06 2.31744634e-05]\n",
            " [7.32903374e-08 3.73751652e-09 2.98139696e-07 3.83453471e-06\n",
            "  2.00932682e-05 1.74047125e-06 9.20894827e-09 3.58003803e-07\n",
            "  4.62991864e-07 9.99973059e-01]\n",
            " [1.91672705e-04 1.44308965e-08 1.58858597e-08 1.78564648e-07\n",
            "  5.95954974e-09 9.58953857e-01 4.08407226e-02 2.22735452e-09\n",
            "  8.79424442e-06 4.72121110e-06]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6m9-B2XMcb3F"
      },
      "source": [
        "Observations\n",
        "---------------------\n",
        "\n",
        "\n",
        "At the first look the results looks good. But did you also observe that we achieved the magic number as well ? If you haven't then go back and see that we achieved it in the 12th Epoch and improved over it on the 15th one . We can also try to see if more number of epochs takes it further up. But before that let's try and focus on the charts to see how it improved over different epochs and how the loss did .\n",
        "\n",
        "\n",
        "\n",
        "This shows that \n",
        "\n",
        "1. The validation loss was always better than the Training loss and \n",
        "2. Simillarly the validation accuracy was always higher than the training accuracy\n",
        "3. The network also narrowed down the difference between the training and validation accuracies \n",
        "\n",
        "That means it did a great job in generalizing the data . We achieved the target with \n",
        "\n",
        "1. Less than 15000 Parameters \n",
        "2. Less than 15 epochs\n",
        "3. WIth no greater than 32 batch size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QexwfPcCPY7p",
        "colab_type": "text"
      },
      "source": [
        "# Connection to Google Drive for saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmcNkw6Tl8fD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "92ba5680-4863-4e9e-8fef-a67888385f1a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji0PLYT0PhnT",
        "colab_type": "text"
      },
      "source": [
        "# Change the root path (if required)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UQ1TgfDmvxC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33cc515a-e9c0-4e47-fe1f-710b5bbbcc6c"
      },
      "source": [
        "\n",
        "root_path = 'gdrive/My Drive/Colab Notebooks/'  #change dir to your project folder"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kZcs9VePlfC",
        "colab_type": "text"
      },
      "source": [
        "# This model is using Image normalization\n",
        "\n",
        "Many times, the photographer with in us asks us to take multiple images of the same object and from many different angles and lighting conditions. What if we take the same image at different time of the day (early morning, afternoon, evening etc.)? Don’t we want out algorithm to identify the object as the same each time? An image taken in bright sun light causes more neurons to fire than the one taken during dusk or after that. \n",
        "\n",
        "Hence one of the most popular ways to solve this problem is by bringing all the images into one scale and then let our model train on those images. So, every time a new image comes, we will convert that to the same scale and feed to our network.\n",
        "\n",
        "![](images/IN.png)\n",
        "\n",
        "\n",
        "We perform image normalization by subtracting the dataset mean and dividing by standard deviation. This serves to \"center\" the data. \n",
        "We do both the things so that in the process of training our network, we're going to be multiplying (weights) and adding to (biases) these initial inputs in order to cause activations that we then backpropagate with the gradients to train the model. This way our gradients will not go out of control.\n",
        "\n",
        "\n",
        "**In the following code we achieve image normalization by **\n",
        "\n",
        "\n",
        "\n",
        "1) Creating an image Data Generator \n",
        "\n",
        "2) Fittingthe model using the Data Generator using a batch size of 512 for 40 epochs \n",
        "\n",
        "finally we \n",
        "\n",
        "Save the Model with highest validation accuracy to google drive for future experiments "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sg33QGHvdLf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f0867b1-4a03-4d5d-8fae-0d9e55d925e8"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(14, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Convolution2D(8, 1, 1, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 5))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "es = EarlyStopping(\"val_acc\",patience=15,restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint(\"model.hdf5\", monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
        "callbacks_list = [es,checkpoint,LearningRateScheduler(scheduler, verbose=1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "#model.compile(loss=custom_loss, optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True,)\n",
        "datagen.fit(X_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=512,shuffle=True),\n",
        "                    steps_per_epoch=int(np.ceil(len(X_train)/512)), \n",
        "                    epochs=40, verbose=1, validation_data=datagen.flow(X_test, Y_test, batch_size=512,shuffle=True), \n",
        "                    validation_steps = int(np.ceil(len(X_test)/512)), \n",
        "                    callbacks=callbacks_list,)\n",
        "\n",
        "\n",
        "\n",
        "#model.fit(X_train, Y_train, batch_size=32, epochs=15, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "\n",
        "model.load_weights(\"model.hdf5\")\n",
        "\n",
        "# steps=np.ceil(len(X_train)/1024)\n",
        "# since it is auto calculated by kera so don't fill  it\n",
        "\n",
        "# iterator = datagen.flow(X_train, Y_train, batch_size=1024, shuffle=False)\n",
        "# score = model.evaluate_generator(iterator,steps=len(iterator))\n",
        "\n",
        "# iterator = datagen.flow(X_train, Y_train, batch_size=1024, shuffle=False)\n",
        "# predictions = model.predict_generator(iterator,steps=len(iterator))\n",
        "\n",
        "# #_ = predictions(score, predictions,Y_train, classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "# iterator = datagen.flow(X_test, Y_test, batch_size=512, shuffle=False)\n",
        "# score = model.evaluate_generator(iterator,steps=len(iterator))\n",
        "\n",
        "iterator = datagen.flow(X_test, Y_test, batch_size=512,shuffle=False)\n",
        "predictions = model.predict_generator(iterator,steps=len(iterator))\n",
        "predicted_class_indices=np.argmax(predictions,axis=1)\n",
        "print(predicted_class_indices[0])\n",
        "\n",
        "#_ = predictions(score, predictions,Y_test, classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "# _=evaluate(model,X_train, Y_train,classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "# _=evaluate(model,X_test, Y_test,classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "# print(score)\n",
        "\n",
        "\n",
        "# y_pred = model.predict(X_test)\n",
        "# print(y_pred[:9])\n",
        "# print(y_test[:9])\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0820 14:52:28.844107 140643382257536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0820 14:52:28.861647 140643382257536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0820 14:52:28.870849 140643382257536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0820 14:52:28.922181 140643382257536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0820 14:52:28.923209 140643382257536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0820 14:52:32.066613 140643382257536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0820 14:52:32.166314 140643382257536 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0820 14:52:32.321444 140643382257536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0820 14:52:32.885572 140643382257536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 14)        1022      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 14)        56        \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 24, 24, 14)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 22, 22, 24)        3048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 8)         200       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 16)          1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 5, 5, 24)          3480      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 5, 5, 24)          96        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 5, 5, 24)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 5, 5, 10)          250       \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 5, 5, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 5, 5, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 1, 1, 10)          2510      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,430\n",
            "Trainable params: 14,254\n",
            "Non-trainable params: 176\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0820 14:52:33.418844 140643382257536 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "118/118 [==============================] - 10s 86ms/step - loss: 0.4125 - acc: 0.8692 - val_loss: 0.1033 - val_acc: 0.9670\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0906 - acc: 0.9725 - val_loss: 0.0617 - val_acc: 0.9801\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0704 - acc: 0.9781 - val_loss: 0.0460 - val_acc: 0.9847\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0582 - acc: 0.9819 - val_loss: 0.0382 - val_acc: 0.9873\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0515 - acc: 0.9840 - val_loss: 0.0388 - val_acc: 0.9878\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0470 - acc: 0.9852 - val_loss: 0.0346 - val_acc: 0.9887\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0430 - acc: 0.9866 - val_loss: 0.0292 - val_acc: 0.9908\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0402 - acc: 0.9870 - val_loss: 0.0276 - val_acc: 0.9912\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0374 - acc: 0.9882 - val_loss: 0.0257 - val_acc: 0.9918\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0356 - acc: 0.9892 - val_loss: 0.0248 - val_acc: 0.9918\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0342 - acc: 0.9890 - val_loss: 0.0246 - val_acc: 0.9921\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0326 - acc: 0.9893 - val_loss: 0.0241 - val_acc: 0.9926\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0321 - acc: 0.9893 - val_loss: 0.0243 - val_acc: 0.9921\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0295 - acc: 0.9907 - val_loss: 0.0228 - val_acc: 0.9926\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0292 - acc: 0.9910 - val_loss: 0.0251 - val_acc: 0.9919\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0279 - acc: 0.9910 - val_loss: 0.0258 - val_acc: 0.9921\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0265 - acc: 0.9916 - val_loss: 0.0236 - val_acc: 0.9925\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0275 - acc: 0.9910 - val_loss: 0.0241 - val_acc: 0.9924\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0275 - acc: 0.9903 - val_loss: 0.0252 - val_acc: 0.9922\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0264 - acc: 0.9916 - val_loss: 0.0237 - val_acc: 0.9932\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0252 - acc: 0.9920 - val_loss: 0.0225 - val_acc: 0.9926\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0258 - acc: 0.9919 - val_loss: 0.0228 - val_acc: 0.9933\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0233 - acc: 0.9923 - val_loss: 0.0216 - val_acc: 0.9931\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0235 - acc: 0.9927 - val_loss: 0.0227 - val_acc: 0.9935\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0226 - acc: 0.9928 - val_loss: 0.0214 - val_acc: 0.9932\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0213 - acc: 0.9931 - val_loss: 0.0221 - val_acc: 0.9939\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0221 - acc: 0.9930 - val_loss: 0.0215 - val_acc: 0.9939\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0207 - acc: 0.9935 - val_loss: 0.0227 - val_acc: 0.9934\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0203 - acc: 0.9934 - val_loss: 0.0231 - val_acc: 0.9922\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0194 - acc: 0.9937 - val_loss: 0.0203 - val_acc: 0.9936\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0201 - acc: 0.9932 - val_loss: 0.0206 - val_acc: 0.9934\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0210 - acc: 0.9933 - val_loss: 0.0221 - val_acc: 0.9937\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0207 - acc: 0.9930 - val_loss: 0.0230 - val_acc: 0.9934\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0201 - acc: 0.9936 - val_loss: 0.0214 - val_acc: 0.9931\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0196 - acc: 0.9936 - val_loss: 0.0212 - val_acc: 0.9939\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0197 - acc: 0.9937 - val_loss: 0.0217 - val_acc: 0.9939\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0195 - acc: 0.9935 - val_loss: 0.0221 - val_acc: 0.9939\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 0.0180 - acc: 0.9942 - val_loss: 0.0220 - val_acc: 0.9935\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0182 - acc: 0.9940 - val_loss: 0.0218 - val_acc: 0.9938\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 0.0187 - acc: 0.9939 - val_loss: 0.0216 - val_acc: 0.9932\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe9b0afc550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaRIb8iV4LY-",
        "colab_type": "text"
      },
      "source": [
        "Comparision of Training and Validation outputs \n",
        "======================================\n",
        "\n",
        "![](images/N1loss.png)\n",
        "![](images/N1acc.png)\n",
        "\n",
        "The Charts above shows that the model is almost generalizing . Having said that we will try other methods to see of anyone helps doing more generalization than this ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV_djY30ffK1",
        "colab_type": "text"
      },
      "source": [
        "# Define Custom Loss function \n",
        "\n",
        "\n",
        "One of the most challenging and much required problem in machine learning is how to make an algorithm that will perform well not just on the training data, but also on new inputs. Many researches and outcomes of those used in machine learning are explicitly designed to reduce the test error, even though at the expense of increased training error. These strategies collectively known as regularization. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Two most popular Regularization methods are L1 & L2 regularization. They are also known as Lasso and Ridge regressions respectively.\n",
        "\n",
        "We can define L1 regularization as \n",
        "\n",
        "![](images/L1_reg.png)\n",
        "\n",
        "\n",
        "where w is the model parameter. That means it’s the sum of absolute values of individual parameters \n",
        "\n",
        "In the similar line L2 regularization can be defined as \n",
        "\n",
        "![](images/L2_reg.png)\n",
        "\n",
        "In this case we use the square of the parameters. It’s also known as weight decay.\n",
        " \n",
        " \n",
        "\n",
        "Below we have defined a method which takes two values  i) the actual values and ii) the predicted value\n",
        "\n",
        "Definee the L2 Regression function\n",
        "\n",
        "Define the value of lambda ( configurable)\n",
        "\n",
        "Calculated the sum of the square of all the weights of all the layers \n",
        "\n",
        "add the L2 Regression value to the Cross entropy loss\n",
        "\n",
        "Returns the value of new loss function which will be used during model fitting and validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M8Wxd_TffXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define custom loss\n",
        "\n",
        "def custom_loss(actual,predicted):\n",
        "    print(actual)\n",
        "    print(predicted)\n",
        "    \n",
        "    sqr_w = 0\n",
        "    lamda = 0.001 \n",
        "\n",
        "    for layer in model.layers:\n",
        "       sqr_w = sqr_w + np.sum(np.sum(np.sum(np.square(layer.get_weights()))))\n",
        "\n",
        "    l2_regularization = (lamda*sqr_w)/2*(32) \n",
        "    loss = K.categorical_crossentropy(actual,predicted) + l2_regularization\n",
        "\n",
        "    # Return a function\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPsp6u7oQDoV",
        "colab_type": "text"
      },
      "source": [
        "# Adding the L2 Regularization with the Loss function\n",
        "\n",
        "So, to want to give a mathematical expression to regularization then it would be something like the below.\n",
        "\n",
        "**J˜(θ;X, y) = J(θ;X, y) + αΩ(θ)**\n",
        "\n",
        "Where J˜: Regularized objective function \n",
        "Ω(θ) – Parameter norm penalty  \n",
        "\n",
        "\n",
        "\"α ∈  0, ∞\"  is a hyperparameter that weights the relative contribution of the norm penalty term, Ω, relative to the standard objective function J.\n",
        "The larger the value of α- The more regularization effect it has. Having said that the data scientists are to be watchful on the extend of αto be used depending on the situation\n",
        "\n",
        "\n",
        "\n",
        "The below model is trained using the custom loss function which is nothing but the combination of cross entropy loss and L2 regularization \n",
        "The model that gave the highest accuracy is saved in the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1DdIKu-R_XF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a1b1023-7802-435e-fdd0-94c7fcf99fcb"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(14, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Convolution2D(8, 1, 1, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 5))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "es = EarlyStopping(\"val_acc\",patience=15,restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint(\"model.hdf5\", monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
        "callbacks_list = [es,checkpoint,LearningRateScheduler(scheduler, verbose=1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "model.compile(loss=custom_loss, optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True,)\n",
        "datagen.fit(X_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=512,shuffle=True),\n",
        "                    steps_per_epoch=int(np.ceil(len(X_train)/512)), \n",
        "                    epochs=40, verbose=1, validation_data=datagen.flow(X_test, Y_test, batch_size=512,shuffle=True), \n",
        "                    validation_steps = int(np.ceil(len(X_test)/512)), \n",
        "                    callbacks=callbacks_list,)\n",
        "\n",
        "\n",
        "\n",
        "#model.fit(X_train, Y_train, batch_size=32, epochs=15, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "\n",
        "model.load_weights(\"model.hdf5\")\n",
        "\n",
        "# steps=np.ceil(len(X_train)/1024)\n",
        "# since it is auto calculated by kera so don't fill  it\n",
        "\n",
        "# iterator = datagen.flow(X_train, Y_train, batch_size=1024, shuffle=False)\n",
        "# score = model.evaluate_generator(iterator,steps=len(iterator))\n",
        "\n",
        "# iterator = datagen.flow(X_train, Y_train, batch_size=1024, shuffle=False)\n",
        "# predictions = model.predict_generator(iterator,steps=len(iterator))\n",
        "\n",
        "# #_ = predictions(score, predictions,Y_train, classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "# iterator = datagen.flow(X_test, Y_test, batch_size=512, shuffle=False)\n",
        "# score = model.evaluate_generator(iterator,steps=len(iterator))\n",
        "\n",
        "iterator = datagen.flow(X_test, Y_test, batch_size=512,shuffle=False)\n",
        "predictions = model.predict_generator(iterator,steps=len(iterator))\n",
        "predicted_class_indices=np.argmax(predictions,axis=1)\n",
        "print(predicted_class_indices[0])\n",
        "\n",
        "#_ = predictions(score, predictions,Y_test, classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "# _=evaluate(model,X_train, Y_train,classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "# _=evaluate(model,X_test, Y_test,classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "# print(score)\n",
        "\n",
        "\n",
        "# y_pred = model.predict(X_test)\n",
        "# print(y_pred[:9])\n",
        "# print(y_test[:9])\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 24, 24, 14)        1022      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 24, 24, 14)        56        \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 24, 24, 14)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 22, 22, 24)        3048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 11, 11, 8)         200       \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 9, 9, 16)          1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 5, 5, 24)          3480      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 5, 5, 24)          96        \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 5, 5, 24)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 5, 5, 10)          250       \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 5, 5, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 5, 5, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 1, 1, 10)          2510      \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,430\n",
            "Trainable params: 14,254\n",
            "Non-trainable params: 176\n",
            "_________________________________________________________________\n",
            "Tensor(\"activation_2_target:0\", shape=(?, ?), dtype=float32)\n",
            "Tensor(\"activation_2/Softmax:0\", shape=(?, ?), dtype=float32)\n",
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "118/118 [==============================] - 8s 67ms/step - loss: 5.0107 - acc: 0.8764 - val_loss: 4.7315 - val_acc: 0.9630\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.7049 - acc: 0.9729 - val_loss: 4.6812 - val_acc: 0.9798\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6808 - acc: 0.9802 - val_loss: 4.6609 - val_acc: 0.9875\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6703 - acc: 0.9835 - val_loss: 4.6535 - val_acc: 0.9873\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6632 - acc: 0.9850 - val_loss: 4.6491 - val_acc: 0.9895\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6601 - acc: 0.9864 - val_loss: 4.6499 - val_acc: 0.9895\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6571 - acc: 0.9873 - val_loss: 4.6465 - val_acc: 0.9902\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6555 - acc: 0.9874 - val_loss: 4.6437 - val_acc: 0.9914\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6527 - acc: 0.9884 - val_loss: 4.6424 - val_acc: 0.9919\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6508 - acc: 0.9893 - val_loss: 4.6438 - val_acc: 0.9913\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6494 - acc: 0.9891 - val_loss: 4.6424 - val_acc: 0.9920\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6472 - acc: 0.9902 - val_loss: 4.6471 - val_acc: 0.9902\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6476 - acc: 0.9897 - val_loss: 4.6414 - val_acc: 0.9912\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6467 - acc: 0.9904 - val_loss: 4.6421 - val_acc: 0.9916\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6456 - acc: 0.9906 - val_loss: 4.6402 - val_acc: 0.9927\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "118/118 [==============================] - 6s 49ms/step - loss: 4.6432 - acc: 0.9914 - val_loss: 4.6407 - val_acc: 0.9928\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6433 - acc: 0.9915 - val_loss: 4.6399 - val_acc: 0.9930\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6417 - acc: 0.9919 - val_loss: 4.6391 - val_acc: 0.9929\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6417 - acc: 0.9916 - val_loss: 4.6393 - val_acc: 0.9929\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6423 - acc: 0.9915 - val_loss: 4.6394 - val_acc: 0.9932\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6405 - acc: 0.9924 - val_loss: 4.6390 - val_acc: 0.9936\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6400 - acc: 0.9925 - val_loss: 4.6373 - val_acc: 0.9938\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 4.6392 - acc: 0.9924 - val_loss: 4.6377 - val_acc: 0.9935\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6391 - acc: 0.9927 - val_loss: 4.6401 - val_acc: 0.9932\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6396 - acc: 0.9924 - val_loss: 4.6379 - val_acc: 0.9935\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6376 - acc: 0.9933 - val_loss: 4.6366 - val_acc: 0.9944\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6386 - acc: 0.9929 - val_loss: 4.6371 - val_acc: 0.9943\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6382 - acc: 0.9929 - val_loss: 4.6383 - val_acc: 0.9936\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6374 - acc: 0.9932 - val_loss: 4.6380 - val_acc: 0.9938\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6373 - acc: 0.9931 - val_loss: 4.6374 - val_acc: 0.9938\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6380 - acc: 0.9924 - val_loss: 4.6372 - val_acc: 0.9941\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6365 - acc: 0.9934 - val_loss: 4.6376 - val_acc: 0.9935\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6366 - acc: 0.9934 - val_loss: 4.6374 - val_acc: 0.9938\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "118/118 [==============================] - 6s 47ms/step - loss: 4.6362 - acc: 0.9935 - val_loss: 4.6363 - val_acc: 0.9942\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6356 - acc: 0.9939 - val_loss: 4.6376 - val_acc: 0.9937\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6353 - acc: 0.9935 - val_loss: 4.6365 - val_acc: 0.9942\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6358 - acc: 0.9938 - val_loss: 4.6374 - val_acc: 0.9938\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6367 - acc: 0.9934 - val_loss: 4.6367 - val_acc: 0.9938\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6356 - acc: 0.9934 - val_loss: 4.6369 - val_acc: 0.9940\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "118/118 [==============================] - 6s 48ms/step - loss: 4.6355 - acc: 0.9933 - val_loss: 4.6363 - val_acc: 0.9940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe96c994e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teer9VGc4tYc",
        "colab_type": "text"
      },
      "source": [
        "Comparision of Training and Validation outputs\n",
        "=======================================\n",
        "\n",
        "![](images/N1loss.png)\n",
        "\n",
        "![](images/N2acc.png)\n",
        "\n",
        "\n",
        "The Charts above shows that the model is improving even though a little. The reason why we are not seeing much difference as MNIST is a easy dataset and we are almost at 99.4 - 99.5 . Even a small improvement on top of this helps in understanding the power of the improvements choosen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dftPBbBJQLGP",
        "colab_type": "text"
      },
      "source": [
        "# By Adding Batch Normalization Before Relu\n",
        "\n",
        "Since the inception of Batch normalization (Y 2015) it's been one of the popular normalization technique which works by eliminating the effect of internal covariation shift. One of the open question is if it should be used before the activation layer or after. There has been many discussions, papers, researches happened in this area , however the benefit of using one first over another is not clearly visible . In the below code we will implement BN before the activation layer and try to see if there are any performance changes over it . Here is a small example of how the graph may look like if we try to \n",
        "\n",
        "i) use BN before relu\n",
        "\n",
        "ii) Relu before BN\n",
        "\n",
        "![](images/BN_relu_tradeoff.png)\n",
        "\n",
        "We don't see any significance difference from the chart theoritically. let's see how the netowrk behaves in this case \n",
        "\n",
        "\n",
        "The model performs the following tasks \n",
        "\n",
        "1. Takes the activation layer outside of the convolution function\n",
        "2. Place Batch normalization function before the activation \n",
        "3. Trains the model for 40 epochs\n",
        "4. Saves the model with highest validation accuracy\n",
        "5. Saves the Model to google drive for future use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPRpY3ZsQPxw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7cf4fb62-c5e7-442f-efe3-0bd1f88f4130"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "#model.add(BatchNormalization()) \n",
        "model.add(Convolution2D(8, 3, 3, input_shape=(28,28,1)))\n",
        "\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(14, 3, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "                        \n",
        "\n",
        "\n",
        "model.add(Convolution2D(24, 3, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Convolution2D(8, 1, 1))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, 3, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))                        \n",
        "model.add(Dropout(0.1))\n",
        "                        \n",
        "\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, 3, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))                          \n",
        "model.add(Dropout(0.1))\n",
        "                        \n",
        "\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Convolution2D(24, 3, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))                        \n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Convolution2D(10, 1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))                        \n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(10, 5))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "es = EarlyStopping(\"val_acc\",patience=15,restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint(\"model.hdf5\", monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
        "callbacks_list = [es,checkpoint,LearningRateScheduler(scheduler, verbose=1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "model.compile(loss=custom_loss, optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True,)\n",
        "datagen.fit(X_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=512,shuffle=True),\n",
        "                    steps_per_epoch=int(np.ceil(len(X_train)/512)), \n",
        "                    epochs=40, verbose=1, validation_data=datagen.flow(X_test, Y_test, batch_size=512,shuffle=True), \n",
        "                    validation_steps = int(np.ceil(len(X_test)/512)), \n",
        "                    callbacks=callbacks_list,)\n",
        "\n",
        "\n",
        "\n",
        "#model.fit(X_train, Y_train, batch_size=32, epochs=15, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "\n",
        "model.load_weights(\"model.hdf5\")\n",
        "\n",
        "# steps=np.ceil(len(X_train)/1024)\n",
        "# since it is auto calculated by kera so don't fill  it\n",
        "\n",
        "# iterator = datagen.flow(X_train, Y_train, batch_size=1024, shuffle=False)\n",
        "# score = model.evaluate_generator(iterator,steps=len(iterator))\n",
        "\n",
        "# iterator = datagen.flow(X_train, Y_train, batch_size=1024, shuffle=False)\n",
        "# predictions = model.predict_generator(iterator,steps=len(iterator))\n",
        "\n",
        "# #_ = predictions(score, predictions,Y_train, classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "# iterator = datagen.flow(X_test, Y_test, batch_size=512, shuffle=False)\n",
        "# score = model.evaluate_generator(iterator,steps=len(iterator))\n",
        "\n",
        "iterator = datagen.flow(X_test, Y_test, batch_size=512,shuffle=False)\n",
        "predictions = model.predict_generator(iterator,steps=len(iterator))\n",
        "predicted_class_indices=np.argmax(predictions,axis=1)\n",
        "print(predicted_class_indices[0])\n",
        "\n",
        "#_ = predictions(score, predictions,Y_test, classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "# _=evaluate(model,X_train, Y_train,classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "# _=evaluate(model,X_test, Y_test,classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "# print(score)\n",
        "\n",
        "\n",
        "# y_pred = model.predict(X_test)\n",
        "# print(y_pred[:9])\n",
        "# print(y_test[:9])\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 24, 24, 14)        1022      \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 24, 24, 14)        56        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 24, 24, 14)        0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 24, 24, 14)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 22, 22, 24)        3048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 22, 22, 24)        96        \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 22, 22, 24)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 11, 11, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 11, 11, 8)         200       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 11, 11, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 9, 9, 16)          1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 5, 5, 24)          3480      \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 5, 5, 24)          96        \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 5, 5, 24)          0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 5, 5, 24)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 5, 5, 10)          250       \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 5, 5, 10)          40        \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 5, 5, 10)          0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 5, 5, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 1, 1, 10)          2510      \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,526\n",
            "Trainable params: 14,302\n",
            "Non-trainable params: 224\n",
            "_________________________________________________________________\n",
            "Tensor(\"activation_11_target:0\", shape=(?, ?), dtype=float32)\n",
            "Tensor(\"activation_11/Softmax:0\", shape=(?, ?), dtype=float32)\n",
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "118/118 [==============================] - 9s 75ms/step - loss: 5.8414 - acc: 0.8535 - val_loss: 5.5147 - val_acc: 0.9527\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.4605 - acc: 0.9680 - val_loss: 5.4298 - val_acc: 0.9774\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.4334 - acc: 0.9777 - val_loss: 5.4196 - val_acc: 0.9818\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.4209 - acc: 0.9806 - val_loss: 5.3957 - val_acc: 0.9873\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.4137 - acc: 0.9829 - val_loss: 5.3878 - val_acc: 0.9903\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.4087 - acc: 0.9843 - val_loss: 5.3897 - val_acc: 0.9906\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.4054 - acc: 0.9850 - val_loss: 5.3836 - val_acc: 0.9912\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.4006 - acc: 0.9863 - val_loss: 5.3823 - val_acc: 0.9926\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3991 - acc: 0.9866 - val_loss: 5.3838 - val_acc: 0.9911\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.3986 - acc: 0.9871 - val_loss: 5.3799 - val_acc: 0.9931\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.3966 - acc: 0.9876 - val_loss: 5.3803 - val_acc: 0.9924\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.3947 - acc: 0.9882 - val_loss: 5.3785 - val_acc: 0.9933\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.3938 - acc: 0.9882 - val_loss: 5.3798 - val_acc: 0.9925\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.3919 - acc: 0.9886 - val_loss: 5.3792 - val_acc: 0.9933\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3910 - acc: 0.9892 - val_loss: 5.3768 - val_acc: 0.9943\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.3899 - acc: 0.9896 - val_loss: 5.3775 - val_acc: 0.9939\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3887 - acc: 0.9898 - val_loss: 5.3788 - val_acc: 0.9933\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.3886 - acc: 0.9899 - val_loss: 5.3782 - val_acc: 0.9934\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3877 - acc: 0.9906 - val_loss: 5.3785 - val_acc: 0.9934\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.3878 - acc: 0.9900 - val_loss: 5.3775 - val_acc: 0.9937\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 5.3879 - acc: 0.9903 - val_loss: 5.3765 - val_acc: 0.9938\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.3865 - acc: 0.9907 - val_loss: 5.3768 - val_acc: 0.9938\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3862 - acc: 0.9903 - val_loss: 5.3768 - val_acc: 0.9934\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3846 - acc: 0.9909 - val_loss: 5.3780 - val_acc: 0.9940\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.3848 - acc: 0.9911 - val_loss: 5.3774 - val_acc: 0.9932\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3845 - acc: 0.9914 - val_loss: 5.3787 - val_acc: 0.9933\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3844 - acc: 0.9914 - val_loss: 5.3776 - val_acc: 0.9935\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.3837 - acc: 0.9914 - val_loss: 5.3765 - val_acc: 0.9945\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3831 - acc: 0.9919 - val_loss: 5.3768 - val_acc: 0.9935\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3833 - acc: 0.9916 - val_loss: 5.3768 - val_acc: 0.9940\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3827 - acc: 0.9917 - val_loss: 5.3768 - val_acc: 0.9940\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3826 - acc: 0.9921 - val_loss: 5.3772 - val_acc: 0.9937\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.3823 - acc: 0.9920 - val_loss: 5.3769 - val_acc: 0.9937\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.3824 - acc: 0.9919 - val_loss: 5.3777 - val_acc: 0.9935\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3835 - acc: 0.9914 - val_loss: 5.3759 - val_acc: 0.9942\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3819 - acc: 0.9922 - val_loss: 5.3757 - val_acc: 0.9947\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3808 - acc: 0.9924 - val_loss: 5.3761 - val_acc: 0.9938\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3809 - acc: 0.9923 - val_loss: 5.3759 - val_acc: 0.9941\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 5.3806 - acc: 0.9924 - val_loss: 5.3766 - val_acc: 0.9937\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 5.3807 - acc: 0.9923 - val_loss: 5.3760 - val_acc: 0.9940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe96cfff630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzCVoseI5IJX",
        "colab_type": "text"
      },
      "source": [
        "Comparision of Training and Validation outputs\n",
        "=========================================\n",
        "\n",
        "![](images/N3loss.png)\n",
        "![](images/N3acc.png)\n",
        "\n",
        "\n",
        "The Charts above shows that the model is generalizing well. The results are not much different than the previous model which helps us to understand that BN before relu doesn't have a large impact on the performance of the network. Below i have pasted the charts to compare the accuracy between the networks. This should give us a good visualization to understand the impact of each feature on the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOh-apwDQRSI",
        "colab_type": "text"
      },
      "source": [
        "# Save the Model to Google Drive for Future Use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO1iHRTMzlPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('gdrive/My Drive/Colab Notebooks/model_final_assignment5.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-yXLCp9QUnb",
        "colab_type": "text"
      },
      "source": [
        "# Display the Misclassified Images (25)\n",
        "\n",
        "Display the first 25 misclassified images\n",
        "\n",
        "The image should include the Predicted Class and Actual Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8h4Mto8ANf8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "outputId": "3eebbbbe-be71-4ac2-a4f3-17d37418b966"
      },
      "source": [
        "%matplotlib inline\n",
        "#y_pred = model.predict_classes(X_test)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28)\n",
        "image_cnt = 0\n",
        "#display_cnt=1\n",
        "w=10\n",
        "h=10\n",
        "fig=plt.figure(figsize=(12, 12))\n",
        "columns = 5\n",
        "rows = 5\n",
        "\n",
        "for i in range(len(predicted_class_indices)):\n",
        " # print('value of i before is',i)\n",
        "  if predicted_class_indices[i] != y_test[i]:\n",
        "    ax = fig.add_subplot(rows, columns, image_cnt+1)\n",
        "    title = 'Pred-' + str(predicted_class_indices[i]) +'Actual-' + str(y_test[i] )\n",
        "   # ax.title.set_text('                      ' )\n",
        "    ax.title.set_text(title )\n",
        "    \n",
        "    plt.imshow(X_test[i])\n",
        "    image_cnt +=1\n",
        "    #display_cnt +=1\n",
        "   # print('value of image count is', image_cnt)  \n",
        "  if image_cnt >= 25:\n",
        "    break\n",
        "    i = len(y_pred)\n",
        "   # print(' length of ypred is',len(y_pred),'and value of i now is',i)\n",
        "\n",
        "plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
        "plt.show()    \n",
        "\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96924e278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a76ec18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a7a0198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a7496d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a6f2c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a6a4198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a6cb710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a675c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a627198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a64f6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a5f8c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a5aa198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a5d36d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a57cc18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a530198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a5576d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a4ffc18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a4b3198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a4da6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a485c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a437198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a462780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a40ac88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a3bf208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96a365748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAK7CAYAAAD8yjntAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x864 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTk8juueQcXY",
        "colab_type": "text"
      },
      "source": [
        "# Find out the Total number of misclassified images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riY37-K0Jjgc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f8fa1c5-4c3b-4604-acff-ae313f01a3ef"
      },
      "source": [
        "#y_pred = model.predict_classes(X_test)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28)\n",
        "count1 =0\n",
        "for i in range(len(predicted_class_indices)):\n",
        " # print('value of i before is',i)\n",
        "  if predicted_class_indices[i] != y_test[i]:\n",
        "    count1 +=1\n",
        "print('count is', count1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count is 53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncKqtVC61zXw",
        "colab_type": "text"
      },
      "source": [
        "Comparision Between the Accuracies of each of the Model\n",
        "=================================================\n",
        "![](images/acc_compare.png)\n",
        "\n",
        "If we look at the charts above it's significant that there are improvements of using Image normalization over the model without.We can also see some small improvement over using a custom loss function. The Accuracies are not drastically high as MNIST is a simple data set and the model generalized to a good extent after normalizing the image. Having said that this small improvement can have a much larger impact when we apply this technique over complicated data. At the same time much more iterations with variable values of regularization parameter may improve the accuracy  and reduce the gap between the training and validation parameters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTNJqank2xuT",
        "colab_type": "text"
      },
      "source": [
        "Comparision Between the Loss for the same loss function while using BN before relu and After relu\n",
        "===============================================================================\n",
        "![](images/loss_compare.png)\n",
        "\n",
        "\n",
        "Now when we see the above charts it's clearly evident that the loss is more while we use BN before relu . But is this a issue that we should address. Certainly not as the loss is due to the weights . When we compare between the loss in Training and validation for the same neetwork it's in the similar range . Let's try to understand why the loss is more in the network where BN is used before Relu.\n",
        "\n",
        "The reason behind that is the output value of the layer. When we use relu after BN it eliminates the negative values and now in the nest layer the kernel which needs to work on it assigns larger weights in comparision to the 2nd network where the BN was applied after activation function in which the output was also negative and smaller weights were good for it .\n",
        "\n",
        "Even though the values are less or more but let's understand that it's all about the scaling factor.\n",
        "\n",
        "I will try to run these experiments more numer of times and update this with my findings .\n",
        "\n",
        "Please reachout to me for any comments /suggestions (nihar.kanungo@gmail.com)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9tpU-DKOIbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
